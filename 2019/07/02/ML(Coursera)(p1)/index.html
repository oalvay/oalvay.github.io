<!DOCTYPE html>
<html lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="browsermode" content="application">
<meta name="apple-touch-fullscreen" content="yes">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="Oalvay's Blog">
<meta name="apple-mobile-web-app-status-bar-style" content="default">
<meta name="msapplication-navbutton-color" content="#666666">
<meta name="format-detection" content="telephone=no">






<link rel="apple-touch-startup-image" media="(device-width: 375px)" href="assets/apple-launch-1125x2436.png">
<link rel="apple-touch-startup-image" media="(orientation: landscape)" href="assets/apple-touch-startup-image-2048x1496.png">
<link rel="stylesheet" href="/style/style.css">
<script>
  var nlviconfig = {
    title: "Oalvay's Blog",
    author: "oalvay",
    baseUrl: "/",
    theme: {
      scheme: "banderole",
      lightbox: true,
      animate: true,
      search: false,
      friends: false,
      reward: false,
      pjax: false,
      lazy: false,
      toc: true
    }
  }
</script>




    <link rel="stylesheet" href="/script/lib/lightbox/css/lightbox.min.css">




    <link rel="stylesheet" href="/syuanpi/syuanpi.min.css">













  <title> Study notes for ML By Coursera (part 1) · Oalvay's Blog </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container" style="display:none;">
    <header class="header" id="header">
  <div class="header-wrapper">
    <div class="logo">
  <div class="logo-inner syuanpi tvIn">
    <h1><a href="/">Oalvay's Blog</a></h1>
    
  </div>
</div>

    <nav class="main-nav">
  
  <ul class="main-nav-list syuanpi tvIn">
  
  
  
    
  
    <li class="menu-item">
      <a href="/archives" id="archives">
        <span class="base-name">
          
            ARCHIVES
          
        </span>
      </a>
    </li>
  
  
    
  
    <li class="menu-item">
      <a href="javascript:;" id="tags">
        <span class="base-name">
          
            TAGS
          
        </span>
      </a>
    </li>
  
  
    
  
    <li class="menu-item">
      <a href="/about" id="about">
        <span class="base-name">
          
            ABOUT
          
        </span>
      </a>
    </li>
  
  
  </ul>
  
</nav>

  </div>
</header>
<div class="mobile-header" id="mobile-header">
  <div class="mobile-header-nav">
    <div class="mobile-header-item" id="mobile-left">
      <div class="header-menu-item">
        <div class="header-menu-line"></div>
      </div>
    </div>
    <h1 class="mobile-header-title">
      <a href="/">Oalvay's Blog</a>
    </h1>
    <div class="mobile-header-item"></div>
  </div>
  <div class="mobile-header-body">
    <ul class="mobile-header-list">
      
        <li class="mobile-nav-item syuanpi fadeInRightShort back-0">
          <a href="/archives">
            
              ARCHIVES
            
          </a>
        </li>
      
        <li class="mobile-nav-item syuanpi fadeInRightShort back-1">
          <a href="javascript:;" id="mobile-tags">
            
              TAGS
            
          </a>
        </li>
      
        <li class="mobile-nav-item syuanpi fadeInRightShort back-2">
          <a href="/about">
            
              ABOUT
            
          </a>
        </li>
      
    </ul>
  </div>
</div>



    <div class="container-inner">
      <main class="main" id="main">
        <div class="main-wrapper">
          
    
  
  <article class="
  post
   is_post 
  ">
    <header class="post-header">
      <div class="post-time syuanpi fadeInRightShort back-1">
        <div class="post-time-wrapper">
          <time>2019-07-02</time>
          
        </div>
      </div>
      <h2 class="post-title syuanpi fadeInRightShort back-2">
        
          Study notes for ML By Coursera (part 1)
        
      </h2>
    </header>
    <div class="post-content syuanpi fadeInRightShort back-3">
      
        <p>This blog contains materials from the course <a href="https://www.coursera.org/learn/machine-learning/" target="_blank">Machine Learning</a> ran by Coursera</p>
<a id="more"></a>
<h2 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h2><p><strong>Supervised learning</strong> problems are categorized into “regression” and “classification” problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.</p>
<p><strong>Unsupervised learning</strong> allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables.</p>
<h3 id="Supervised-learning-—-Regression"><a href="#Supervised-learning-—-Regression" class="headerlink" title="Supervised learning — Regression"></a>Supervised learning — Regression</h3><p>We can derive this structure by clustering the data based on relationships among the variables in the data.</p>
<script type="math/tex; mode=display">\begin{align*}
m &= \text{Number of training examples} \newline 
x's &= \text{"input" variables / features}  \newline
y's &= \text{"output" variables / "target" variable} \newline
(x^{i}, y^{i}) \ &= i_{th} \ \text{training example} \newline
\end{align*}</script><p>To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a <strong>hypothesis</strong>.</p>
<p><strong>cost function</strong> (Mean squared error MSE):</p>
<script type="math/tex; mode=display">J(\theta_0, ..., \theta_n)  = \dfrac {1}{2m} \displaystyle \sum _{i=1}^m \left (h_\theta (x_{i}) - y_{i} \right)^2</script><p>Our goal is to minimise the cost function, where one possible approach is <strong>gradient descent</strong>:  </p>
<script type="math/tex; mode=display">\theta_j := \theta_j - \frac{\alpha}{m} \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)</script><p>where j = 0, …, n  and α is learning rate. </p>
<p>We keep update θ’s simultaneously until it converge, such that MSE is minimised.<br>One of the potential issues with this approach is that it may diverge or converge to local minimum(which is not global minimum).</p>
<h2 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h2><h3 id="Multivariate-Linaer-Regression"><a href="#Multivariate-Linaer-Regression" class="headerlink" title="Multivariate Linaer Regression"></a>Multivariate Linaer Regression</h3><script type="math/tex; mode=display">\begin{align*}x_j^{(i)} &= \text{value of feature } j \text{ in the }i^{th}\text{ training example} \newline x^{(i)}& = \text{the input (features) of the }i^{th}\text{ training example} \newline m &= \text{the number of training examples} \newline n &= \text{the number of features} \end{align*}</script><p><strong>hypothesis</strong>: <script type="math/tex">\begin{align*}h_\theta(x) &=\begin{bmatrix}\theta_0 \hspace{1em} \theta_1 \hspace{1em} ... \hspace{1em} \theta_n\end{bmatrix}\begin{bmatrix}x_0 \newline x_1 \newline \vdots \newline x_n\end{bmatrix}= \theta^T x \newline
&= \theta_0 + \theta_1x_1 + ... + \theta_nx_n
\end{align*}</script>  </p>
<p><strong>Gradient Descent for Multiple Variables</strong>:</p>
<script type="math/tex; mode=display">\begin{align*}& \text{repeat until convergence:} \; \lbrace \newline \; & \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \; & \text{for j := 0...n}\newline \rbrace\end{align*}</script><p>There are some techiques that could spped up Gradient Descent: </p>
<ul>
<li><strong>feature scaling</strong> and <strong>mean normalization</strong> — (working with <strong>features</strong>)</li>
<li><p><strong>Debugging gradient descent</strong>: Make a plot with number of iterations on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α.</p>
<p><strong>Automatic convergence test</strong>: Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such<br>as 0.0001. However in practice it’s difficult to choose this threshold value.  </p>
<p>— (working with <strong>learning rate</strong>)</p>
</li>
</ul>
<p>On other hand, we can <span style="border-bottom:1.5px solid black;">combine multiple features into one</span>. For example, we can combine features a and b into a new feature c by taking their product.</p>
<h3 id="Polynomial-Regression"><a href="#Polynomial-Regression" class="headerlink" title="Polynomial Regression"></a>Polynomial Regression</h3><p>Our hypothesis function need not be linear (a straight line) if that does not fit the data well.</p>
<p>We can <strong>change the behavior or curve</strong> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).</p>
<p>For instance, we could do <script type="math/tex">h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2\sqrt{x_1} + \theta_3x_1^2</script></p>
<h2 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h2><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p>The classification problem is just like the regression problem, except that the values we now want to predict take on only a finite number of discrete values. </p>
<p>Using Linear regression for classification is often not a very good idea, so we need a new learning method for this kind of problem.</p>
<p>This course will focus on <strong>binary classification problem</strong>, where y only takes 2 values, say 0 or 1.</p>
<h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>Our new form uses the <strong>Sigmoid Function</strong>, also called the <strong>Logistic Function</strong>:</p>
<script type="math/tex; mode=display">\begin{align*} h_\theta (x)& = g ( \theta^T x ) \newline \text{where: } \newline g(z) &= \dfrac{1}{1 + e^{-z}} \newline z &= \theta^T x \end{align*}</script><p>such that the hypothesis function now satisfies 0 ≤ h ≤ 1, and represents the <strong>probability</strong> of the output being 1:</p>
<script type="math/tex; mode=display">\begin{align*}& h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \end{align*}</script><p>Our hypothesis function will create a <strong>decision boundary</strong>, which is a line that separates the area where y = 0 and where y = 1</p>
<p>Again, like Polynomial Regression, the input to the sigmoid function can be non-linear.(so the decision boundary may be a open/closed curve instead)</p>
<h4 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h4><p>We cannot use the same cost function that we use for linear regression because the Logistic Function will <span style="border-bottom:1.5px solid black;">cause the output to be wavy, causing many local optima</span>. In other words, it will not be a convex function, where there only exists one global minimum.</p>
<p>Instead, our cost function for logistic regression looks like:</p>
<script type="math/tex; mode=display">\begin{align*}& J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) 
\newline& \text{where the cost for indiviual training example is:} \newline
\newline & \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; & \text{if y = 1} 
\newline & \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; & \text{if y = 0}\end{align*}</script><p>And these two functions looks like:<br><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Q9sX8nnxEeamDApmnD43Fw_1cb67ecfac77b134606532f5caf98ee4_Logistic_regression_cost_function_positive_class.png?expiry=1562544000000&hmac=ILxM5W3MAPSvYreza1emsVesWz9GThHEImzYmyZTX1I" width="33%"> &emsp; &emsp; &emsp; <img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Ut7vvXnxEead-BJkoDOYOw_f719f2858d78dd66d80c5ec0d8e6b3fa_Logistic_regression_cost_function_negative_class.png?expiry=1562544000000&hmac=Io_ytMylorZtX9nHNA_HuGDzaqyRc5HcEyOkaPkDnGs" width="30%"></p>
<p>so that these have the following properties:</p>
<script type="math/tex; mode=display">\begin{align*}& \mathrm{Cost}(h_\theta(x),y) = 0 \text{ if } h_\theta(x) = y \newline & \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 0 \; \mathrm{and} \; h_\theta(x) \rightarrow 1 \newline & \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 1 \; \mathrm{and} \; h_\theta(x) \rightarrow 0 \newline \end{align*}</script><p>And actually, we can compress our cost function’s two conditional cases into one case:</p>
<script type="math/tex; mode=display">\mathrm{Cost}(h_\theta(x),y) = - y \; \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))</script><p>Therefore our entire cost function is as follows:</p>
<script type="math/tex; mode=display">\begin{align*} J(\theta) = - \dfrac{1}{m} \sum_{i=1}^m [y^{(i)}\log(h_\theta(x^{(i)})) + (1 - y^{(i)})\log(1 - h_\theta(x^{(i)}))] \end{align*}</script><p>Or, in vectorized implementation:</p>
<script type="math/tex; mode=display">\begin{align*} &  J(\theta) = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right) \newline &\text{where} \; \; h = g(X\theta)\end{align*}</script><h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><p>The Gradient Descent procedure for logistic regression looks suprisingly similar with linear regression:</p>
<script type="math/tex; mode=display">\begin{align*} & Repeat \; \lbrace  \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}  \rbrace \end{align*}</script><p>Or, in vectorized implementation:</p>
<script type="math/tex; mode=display">\begin{align*}  \; \theta := \theta - \frac{\alpha}{m} X^{T}(g(X\theta) - y) \end{align*}</script><h4 id="Advanced-optimization-techniques"><a href="#Advanced-optimization-techniques" class="headerlink" title="Advanced optimization techniques"></a>Advanced optimization techniques</h4><p>There are more sophisticated, faster ways to optimize θ that can be used instead of gradient descent, eg:</p>
<ul>
<li>Conjugate gradient</li>
<li>BFGS</li>
<li>L-BFGS</li>
</ul>
<p>To use these </p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[jVal, gradient]</span> = <span class="title">costFunction</span><span class="params">(theta)</span></span></span><br><span class="line">  jVal = [...code to compute J(theta)...];</span><br><span class="line">  gradient = [...code to compute derivative of J(theta)...];</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure>
<p>Then we can use octave’s <code>fminunc()</code> optimization algorithm along with the <code>optimset()</code> function that creates an object containing the options we want to send to <code>fminunc()</code>:</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, <span class="number">100</span>);</span><br><span class="line">initialTheta = <span class="built_in">zeros</span>(<span class="number">2</span>,<span class="number">1</span>);</span><br><span class="line">   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);</span><br></pre></td></tr></table></figure>
<h3 id="Multiclass-Classification"><a href="#Multiclass-Classification" class="headerlink" title="Multiclass Classification"></a>Multiclass Classification</h3><p>For now we have y = {0,1…n}, one way we can choose is to divide our problem into n+1 binary classification problems; in each one, we choose one class and then lumping all the others into a single second class and then applying binary logistic regression, and then use the hypothesis that returned the highest value  (probability)as our prediction, namely pick the class that maximizes the hypothesises, mathematically:</p>
<script type="math/tex; mode=display">\begin{align*}& y \in \lbrace0, 1 ... n\rbrace \newline& h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \newline& h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \newline& \cdots \newline& h_\theta^{(n)}(x) = P(y = n | x ; \theta) \newline& \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\newline\end{align*}</script><p>The following image shows an example of 3 classes:</p>
<p><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/cqmPjanSEeawbAp5ByfpEg_299fcfbd527b6b5a7440825628339c54_Screenshot-2016-11-13-10.52.29.png?expiry=1562544000000&hmac=4ylSpL-hmzsFt0E7neP_a7mQ3w3cDTMIgDz9lSMlo7c" width="80%"></p>
<h3 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h3><p>Overfitting is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. This terminology is applied to both linear and logistic regression. There are two main options to address the issue of overfitting:</p>
<p>(1) Reduce the number of features</p>
<ul>
<li>Manually select which features to keep.</li>
<li>Use a model selection algorithm (studied later in the course).</li>
</ul>
<h4 id="2-Regularization"><a href="#2-Regularization" class="headerlink" title="(2)Regularization"></a>(2)Regularization</h4><ul>
<li>Keep all the features, but reduce the magnitude of parameters </li>
<li>Regularization works well when we have a lot of slightly useful features.</li>
</ul>
<p>The way that to reduce the influence of parameters is to modify our cost function by introducing an additonal term:</p>
<script type="math/tex; mode=display">\min_\theta \frac{1}{2m} \sum_{i = 1}^{m} ((h_{\theta}(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{i = 1}^n \theta_j^2)</script><p>where λ is the regularization parameter. It determines how much the costs of our theta parameters are inflated.</p>
<p>Or, if we want to eliminate a few parameters instead of all of them, just simply  inflate the cost of those parameters. For example:</p>
<p><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/j0X9h6tUEeawbAp5ByfpEg_ea3e85af4056c56fa704547770da65a6_Screenshot-2016-11-15-08.53.32.png?expiry=1562630400000&hmac=tknbEM7AvywnJfZgsMawWFrWkVReJ9tx9KBvuBT4inw" width="80%"></p>
<p>We can apply regularization to both linear regression and logistic regression. We will approach linear regression first:</p>
<h5 id="Regularized-Linear-Regression"><a href="#Regularized-Linear-Regression" class="headerlink" title="Regularized Linear Regression"></a>Regularized Linear Regression</h5><p>For gradient descent, we will modify our gradient descent function:</p>
<script type="math/tex; mode=display">\begin{align*} & \text{Repeat}\ \lbrace \newline & \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \ \ (\text{no need to penalize }\theta_0)
\newline & \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline & \rbrace \end{align*}</script><p>The new added term performs our regularization. We can also write the function as:</p>
<script type="math/tex; mode=display">\theta_j := \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha   \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}</script><p>The first term in the above equation, 1 - αλ/m will always be less than 1. Intuitively you can see it as reducing the value of by some amount on every update. Notice that the second term is now exactly the same as it was before.</p>
<p>For Normal Equation, please refer to the <a href="https://www.coursera.org/learn/machine-learning/supplement/pKAsc/regularized-linear-regression" target="_blank">reading page</a> for details.</p>
<h5 id="Regularized-Logistic-Regression"><a href="#Regularized-Logistic-Regression" class="headerlink" title="Regularized Logistic Regression"></a>Regularized Logistic Regression</h5><p>Regularization for logistic regression is similar to the previous one, where we introduce the exactly same term to the cost function as before:</p>
<script type="math/tex; mode=display">J(\theta) = - \frac{1}{m} \sum_{i = 1}^m \left[ y^{(i)}\log(h_\theta(x^{(i)})) + (1 - y^{(i)})\log(1 - h_\theta(x^{(i)}))\right] + \frac{\lambda}{2m} \sum_{i = 1}^n \theta_j^2 \ \ \ (\theta_0\text{ is skipped})</script><p>and then the gradient descent function (procedure) looks exactly the same as the one for linear regression.</p>

      
    
    </div>
    
    
      

      
    
  </article>
  
    
  <nav class="article-page">
    
      <a href="/2019/07/03/tips-for-MATLAB/" id="art-left" class="art-left">
        <span class="next-title">
          <i class="iconfont icon-back"></i>MATLAB related
        </span>
      </a>
    
    
      <a href="/2019/06/17/hexo-github-搭建博客小记/" id="art-right" class="art-right">
        <span class="prev-title">
          hexo+github 搭建博客小记<i class="iconfont icon-enter"></i>
        </span>
      </a>
    
  </nav>


    
  <i id="com-switch" class="iconfont icon-more jumping-in long infinite" style="font-size:24px;display:block;text-align:center;transform:rotate(180deg);"></i>
  <div class="post-comments" id="post-comments" style="display: block;margin: auto 16px;">
    
    
    

    

  </div>



  
  
    
  
  <aside class="post-toc">
    <div class="title"><span>Index</span></div>
    <div class="toc-inner">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Week-1"><span class="toc-text">Week 1</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Supervised-learning-—-Regression"><span class="toc-text">Supervised learning — Regression</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Week-2"><span class="toc-text">Week 2</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Multivariate-Linaer-Regression"><span class="toc-text">Multivariate Linaer Regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Polynomial-Regression"><span class="toc-text">Polynomial Regression</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Week-3"><span class="toc-text">Week 3</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification"><span class="toc-text">Classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Logistic-Regression"><span class="toc-text">Logistic Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Cost-function"><span class="toc-text">Cost function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Gradient-Descent"><span class="toc-text">Gradient Descent</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Advanced-optimization-techniques"><span class="toc-text">Advanced optimization techniques</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multiclass-Classification"><span class="toc-text">Multiclass Classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Overfitting"><span class="toc-text">Overfitting</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Regularization"><span class="toc-text">(2)Regularization</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Regularized-Linear-Regression"><span class="toc-text">Regularized Linear Regression</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Regularized-Logistic-Regression"><span class="toc-text">Regularized Logistic Regression</span></a></li></ol></li></ol></li></ol></li></ol>
    </div>
  </aside>



  


        </div>
      </main>
      <footer class="footer syuanpi fadeIn" id="footer">
  <hr>
  <div class="footer-wrapper">
    <div class="left">
      <div class="contact-icon">
    
    
    
    
    
    
    
    
    
        
            <a href="https://github.com/oalvay" class="iconfont icon-social-github" title="github"></a>
        
        
        
        
        
        
        
        
    
</div>

    </div>
    <div class="right">
      <div class="copyright">
    <div class="info">
        <span>&copy;</span>
        <span>2019 ~ 2019</span>
        <span>❤</span>
        <span>oalvay</span>
    </div>
    <div class="theme">
        <span>
            Powered by
            <a href="http://hexo.io/" target="_blank">Hexo </a>
        </span>
        <span>
            Theme
            <a href="https://github.com/ColMugX/hexo-theme-Nlvi"> Nlvi </a>
        </span>
    </div>
    
</div>

    </div>
  </div>
</footer>
    </div>
    <div class="tagcloud" id="tagcloud">
  <div class="tagcloud-taglist">
  
    <div class="tagcloud-tag">
      <button>主线剧情</button>
    </div>
  
    <div class="tagcloud-tag">
      <button>技多不压身</button>
    </div>
  
  </div>
  
    <div class="tagcloud-postlist active">
      <h2>主线剧情</h2>
      
        <div class="tagcloud-post">
          <a href="/2019/06/17/hello-world/">
            <time class="tagcloud-posttime">2019 / 06 / 17</time>
            <span>Hello Hexo</span>
          </a>
        </div>
      
    </div>
  
    <div class="tagcloud-postlist ">
      <h2>技多不压身</h2>
      
        <div class="tagcloud-post">
          <a href="/2019/06/17/hexo-github-搭建博客小记/">
            <time class="tagcloud-posttime">2019 / 06 / 17</time>
            <span>hexo+github 搭建博客小记</span>
          </a>
        </div>
      
    </div>
  
</div>

  </div>
  <div class="backtop syuanpi melt toTop" id="backtop">
    <i class="iconfont icon-up"></i>
    <span style="text-align:center;font-family:Georgia;"><span style="font-family:Georgia;" id="scrollpercent">1</span>%</span>
</div>


<script src="/script/lib/jquery/jquery-3.2.1.min.js"></script>


  <script src="/script/lib/lightbox/js/lightbox.min.js"></script>



  <script src="https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/x-mathjax-config;executed=true">MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"], ["\\(","\\)"]]}});</script>







  <script src="/script/scheme/banderole.js"></script>


<script src="/script/bootstarp.js"></script>


<script>
if (nlviconfig.theme.toc) {
  setTimeout(function() {
    if (nlviconfig.theme.scheme === 'balance') {
      $("#header").addClass("show_toc");
    } else if (nlviconfig.theme.scheme === 'banderole') {
      $(".container-inner").addClass("has_toc");
      $(".post-toc .title").addClass("show");
      $(".toc-inner").addClass("show");
    }
  }, 1000);
}
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



</body>
</html>
