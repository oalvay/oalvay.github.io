<!DOCTYPE html>
<html lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="browsermode" content="application">
<meta name="apple-touch-fullscreen" content="yes">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="Oalvay's Blog">
<meta name="apple-mobile-web-app-status-bar-style" content="default">
<meta name="msapplication-navbutton-color" content="#666666">
<meta name="format-detection" content="telephone=no">





  <meta name="keywords" content="Machine learning Andrew Ng, nlvi">


<link rel="apple-touch-startup-image" media="(device-width: 375px)" href="assets/apple-launch-1125x2436.png">
<link rel="apple-touch-startup-image" media="(orientation: landscape)" href="assets/apple-touch-startup-image-2048x1496.png">
<link rel="stylesheet" href="/style/style.css">
<script>
  var nlviconfig = {
    title: "Oalvay's Blog",
    author: "oalvay",
    baseUrl: "/",
    theme: {
      scheme: "banderole",
      lightbox: true,
      animate: true,
      search: false,
      friends: false,
      reward: false,
      pjax: false,
      lazy: false,
      toc: true
    }
  }
</script>




    <link rel="stylesheet" href="/script/lib/lightbox/css/lightbox.min.css">




    <link rel="stylesheet" href="/syuanpi/syuanpi.min.css">













  <title> Notes for Coursera Machine Learning Week 6 · Oalvay's Blog </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container" style="display:none;">
    <header class="header" id="header">
  <div class="header-wrapper">
    <div class="logo">
  <div class="logo-inner syuanpi tvIn">
    <h1><a href="/">Oalvay's Blog</a></h1>
    
  </div>
</div>

    <nav class="main-nav">
  
  <ul class="main-nav-list syuanpi tvIn">
  
  
  
    
  
    <li class="menu-item">
      <a href="/about" id="about">
        <span class="base-name">
          
            ABOUT
          
        </span>
      </a>
    </li>
  
  
    
  
    <li class="menu-item">
      <a href="javascript:;" id="tags">
        <span class="base-name">
          
            TAGS
          
        </span>
      </a>
    </li>
  
  
    
  
    <li class="menu-item">
      <a href="/archives" id="archives">
        <span class="base-name">
          
            ARCHIVES
          
        </span>
      </a>
    </li>
  
  
  </ul>
  
</nav>

  </div>
</header>
<div class="mobile-header" id="mobile-header">
  <div class="mobile-header-nav">
    <div class="mobile-header-item" id="mobile-left">
      <div class="header-menu-item">
        <div class="header-menu-line"></div>
      </div>
    </div>
    <h1 class="mobile-header-title">
      <a href="/">Oalvay's Blog</a>
    </h1>
    <div class="mobile-header-item"></div>
  </div>
  <div class="mobile-header-body">
    <ul class="mobile-header-list">
      
        <li class="mobile-nav-item syuanpi fadeInRightShort back-0">
          <a href="/about">
            
              ABOUT
            
          </a>
        </li>
      
        <li class="mobile-nav-item syuanpi fadeInRightShort back-1">
          <a href="javascript:;" id="mobile-tags">
            
              TAGS
            
          </a>
        </li>
      
        <li class="mobile-nav-item syuanpi fadeInRightShort back-2">
          <a href="/archives">
            
              ARCHIVES
            
          </a>
        </li>
      
    </ul>
  </div>
</div>



    <div class="container-inner">
      <main class="main" id="main">
        <div class="main-wrapper">
          
    
  
  <article class="
  post
   is_post 
  ">
    <header class="post-header">
      <div class="post-time syuanpi fadeInRightShort back-1">
        <div class="post-time-wrapper">
          <time>2019-07-16</time>
          
        </div>
      </div>
      <h2 class="post-title syuanpi fadeInRightShort back-2">
        
          Notes for Coursera Machine Learning Week 6
        
      </h2>
    </header>
    <div class="post-content syuanpi fadeInRightShort back-3">
      
        <p>Contents: Evaluation and diagnosis for hypothesises.</p>
<a id="more"></a>
<h2 id="Week-6"><a href="#Week-6" class="headerlink" title="Week 6"></a>Week 6</h2><h3 id="Evaluating-a-Hypothesis"><a href="#Evaluating-a-Hypothesis" class="headerlink" title="Evaluating a Hypothesis"></a>Evaluating a Hypothesis</h3><p>A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a training set and a test set. Typically, the training set consists of 70% of your data and the test set is the remaining 30%.</p>
<p>The new procedure using these two sets is then:</p>
<ol>
<li>Learn <script type="math/tex">Θ</script> and minimize <script type="math/tex">J_{train}(\Theta)</script> using the training set</li>
<li>Compute the test set error <script type="math/tex">J_{test}(\Theta)</script></li>
</ol>
<h4 id="The-test-set-error"><a href="#The-test-set-error" class="headerlink" title="The test set error"></a>The test set error</h4><ul>
<li>For linear regression: </li>
</ul>
<script type="math/tex; mode=display">J_{test}(\Theta) = \dfrac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_\Theta(x^{(i)}_{test}) - y^{(i)}_{test})^2</script><ul>
<li>For classification ~ Misclassification error (aka 0/1 misclassification error):</li>
</ul>
<script type="math/tex; mode=display">err(h_\Theta(x),y) = \begin{matrix} 1 & \mbox{if } h_\Theta(x) \geq 0.5\ and\ y = 0\ or\ h_\Theta(x) < 0.5\ and\ y = 1\newline 0 & \mbox otherwise \end{matrix}</script><p>which gives us a binary 0 or 1 error result based on a misclassification. The average test error for the test set is: </p>
<script type="math/tex; mode=display">\text{Test Error} = \dfrac{1}{m_{test}} \sum^{m_{test}}_{i=1} err(h_\Theta(x^{(i)}_{test}), y^{(i)}_{test})</script><p>This gives us the proportion of the test data that was misclassified.</p>
<h4 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h4><p>I personally think the note and video for this section is not helpful and has done my own research/study separately, one may interested shall refer to <a href="https://www.coursera.org/learn/machine-learning/supplement/XHQqO/model-selection-and-train-validation-test-sets" target="_blank">the materials</a>.</p>
<h3 id="Diagnosing-Bias-vs-Variance"><a href="#Diagnosing-Bias-vs-Variance" class="headerlink" title="Diagnosing Bias vs. Variance"></a>Diagnosing Bias vs. Variance</h3><p>In this section the lecturer examine the relationship between the degree of the polynomial d and the underfitting or overfitting of our hypothesis:</p>
<ul>
<li>One need to distinguish whether <strong>bias</strong> or <strong>variance</strong> is the problem contributing to bad predictions.</li>
<li><span style="border-bottom:1.5px solid black;">High bias is underfitting and high variance is overfitting</span>. Ideally, we need to find a golden mean between these two.</li>
</ul>
<p><img src="https://blog.grio.com/wp-content/uploads/2017/03/bias-variance.png" width="100%"></p>
<p>The training error will tend to <span style="border-bottom:1.5px solid black;">decrease</span> as we increase the degree d of the polynomial.</p>
<p>At the same time, the cross validation error will tend to <strong>decrease</strong> as we increase d up to a point, and then it will <strong>increase</strong> as d is increased, forming a convex curve.</p>
<p><strong>High bias (underfitting)</strong>: both <script type="math/tex">J_{train}(\Theta)</script> and <script type="math/tex">J_{CV}(\Theta)</script> will be high. Also, <script type="math/tex">J_{train}(\Theta) \approx J_{CV}(\Theta)</script></p>
<p><strong>High variance (overfitting)</strong>: <script type="math/tex">J_{train}(\Theta)</script> will be low and <script type="math/tex">J_{CV}(\Theta)</script> will be much greater than <script type="math/tex">J_{train}(\Theta)</script></p>
<p>The is summarized in the figure below:</p>
<p><img src="https://www.learnopencv.com/wp-content/uploads/2017/02/Bias-Variance-Tradeoff-In-Machine-Learning-1.png" width="60%"></p>
<h4 id="Regularization-and-Bias-Variance"><a href="#Regularization-and-Bias-Variance" class="headerlink" title="Regularization and Bias/Variance"></a>Regularization and Bias/Variance</h4><p>As λ increases, our fit is like to become more rigid. On the other hand, as λ approaches 0, we tend to over overfit the data. So how do we choose our parameter λ to get it ‘just right’ ? In order to choose the model and the regularization term λ, we need to:</p>
<ol>
<li>Create a list of lambdas (i.e. λ∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24});</li>
<li>Create a set of models with different degrees or any other variants.</li>
<li>Iterate through the λs and for each λ go through all the models to learn some Θ.</li>
<li>Compute the cross validation error using the learned Θ (computed with λ) on the <script type="math/tex">J_{CV}(Θ)</script> <strong>without</strong> regularization or λ = 0 (as you have regularized already).</li>
<li>Select the best combo that produces the lowest error on the cross validation set.</li>
<li>Using the best combo Θ and λ, apply it on <script type="math/tex">J_{test}(Θ)</script> to see if it has a good generalization of the problem.</li>
</ol>
<h4 id="Learning-Curves"><a href="#Learning-Curves" class="headerlink" title="Learning Curves"></a>Learning Curves</h4><p>Experiencing high bias:</p>
<p>Low training set size: causes <script type="math/tex">J_{train}(Θ)</script> to be low and <script type="math/tex">J_{CV}(Θ)</script> to be high.<br>Large training set size: causes both <script type="math/tex">J_{train}(Θ)</script> and <script type="math/tex">J_{CV}(Θ)</script> to be high with <script type="math/tex">J_{train}(\Theta) \approx J_{CV}(\Theta)</script></p>
<p>If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much:</p>
<p><img src="https://blog.grio.com/wp-content/uploads/2017/03/high-bias-learning-curve.png" width="80%"></p>
<p>Experiencing high variance:</p>
<p>Low training set size: causes <script type="math/tex">J_{train}(Θ)</script> to be low and <script type="math/tex">J_{CV}(Θ)</script> to be high.<br>Large training set size: <script type="math/tex">J_{train}(Θ)</script> increases with training set size and <script type="math/tex">J_{CV}(Θ)</script> continues to decrease without leveling off. Also, <script type="math/tex">J_{train}(\Theta) < J_{CV}(\Theta)</script> but the difference between them remains significant:</p>
<p>If a learning algorithm is suffering from high variance, getting more training data is likely to help.</p>
<p><img src="https://blog.grio.com/wp-content/uploads/2017/03/high-variance-learning-curve.png" width="80%"></p>
<h4 id="Diagnosing-Neural-Networks"><a href="#Diagnosing-Neural-Networks" class="headerlink" title="Diagnosing Neural Networks"></a>Diagnosing Neural Networks</h4><ul>
<li>A neural network with fewer parameters is <strong>prone to underfitting</strong>. It is also <strong>computationally cheaper</strong>.</li>
<li>A large neural network with more parameters is <strong>prone to overfitting</strong>. It is also <strong>computationally expensive</strong>. In this case you can use regularization (increase λ) to address the overfitting.</li>
</ul>
<p>Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using your cross validation set. You can then select the one that performs best.</p>
<h4 id="Overall"><a href="#Overall" class="headerlink" title="Overall"></a>Overall</h4><p>Our decision process can be broken down as follows:</p>
<ul>
<li><strong>Getting more training examples</strong>: Fixes high variance</li>
<li><strong>Trying smaller sets of features</strong>: Fixes high variance</li>
<li><strong>Adding features</strong>: Fixes high bias</li>
<li><strong>Adding polynomial features</strong>: Fixes high bias</li>
<li><strong>Decreasing λ</strong>: Fixes high bias</li>
<li><strong>Increasing λ</strong>: Fixes high variance.</li>
</ul>
<h5 id="Model-Complexity-Effects"><a href="#Model-Complexity-Effects" class="headerlink" title="Model Complexity Effects:"></a>Model Complexity Effects:</h5><ul>
<li>Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.</li>
<li>Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance. </li>
<li>In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.</li>
</ul>
<h3 id="Prioritizing-What-to-Work-On"><a href="#Prioritizing-What-to-Work-On" class="headerlink" title="Prioritizing What to Work On"></a>Prioritizing What to Work On</h3><p>System Design Example:</p>
<p>Given a data set of emails, we could construct a vector for each email in which each entry represents a word. The vector normally contains massive entries gathered by finding the most frequently used words in our data set. A word that exists in the email shall assign its entry to 1 and 0 otherwise. Then we train our algorithm and try to use it to classify if an email is a spam or not.</p>
<p>So how could you spend your time to improve the accuracy of this classifier?</p>
<ul>
<li>Collect lots of data (for example <span style="border-bottom:1.5px solid black;">“honeypot” project</span> but doesn’t always work)</li>
<li>Develop sophisticated features (for example: using email header data in spam emails)</li>
<li>Develop algorithms to process your input in different ways (recognizing misspellings in spam).</li>
</ul>
<p>It is difficult to tell which of the options will be most helpful.</p>
<h4 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h4><p>The recommended approach to solving machine learning problems is to:</p>
<ul>
<li>Start with a simple algorithm, implement it quickly, and test it early on your cross validation data.</li>
<li>Plot learning curves to decide if more data, more features, etc. are likely to help.</li>
<li>Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made.</li>
</ul>
<p>For example, assume that we have 500 emails and our algorithm misclassifies a 100 of them. We could manually analyze the 100 emails and then try to come up with new cues and features that would help us classify these 100 emails correctly.  </p>
<p><strong>It is very important to get error results as a single, numerical value.</strong> Otherwise it is difficult/time-consuming to assess your algorithm’s performance.<br>For example if we try to distinguish between upper and lower case letters and end up getting a 3.2% error rate instead of 3%, then we should avoid using this new feature. Hence, we should try new things, get a numerical performance rate, and based on that decide whether to keep the new feature or not.</p>
<h4 id="Precision-and-Recall"><a href="#Precision-and-Recall" class="headerlink" title="Precision and Recall"></a>Precision and Recall</h4><p>For skewed data, we are often unable to use tools like error rates to assess a model’s performance. That is because, for instance, if we have a dataset in which 0.5% of them have response 0 and 1 otherwise. Then a complicated model with 1% error rate looks not so good as we can simply predict all of them as 1(which gives a 0.5% error rates). It’s also hard to tell whether a improvement for a model, such as 0.8% to 0.5%, is significant or not.<br>We therefore need to develop a new tool for this extreme case, that is the Precision and Recall:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Actually True</th>
<th>Actually False</th>
</tr>
</thead>
<tbody>
<tr>
<td>Predicted True</td>
<td>True positive</td>
<td>False positive</td>
</tr>
<tr>
<td>Predicted False</td>
<td>False negative</td>
<td>True negative</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">\text{Precision} = \frac{\text{True positives}}{\text{No. of predicted as positive}} = \frac{\text{True positives}}{\text{True positives + False positives}}</script><script type="math/tex; mode=display">\text{Recall} = \frac{\text{True positives}}{\text{No. of actual positives}} = \frac{\text{True positives}}{\text{True positives + False negatives}}</script><h5 id="Trade-off-between-Precision-and-Recall"><a href="#Trade-off-between-Precision-and-Recall" class="headerlink" title="Trade off between Precision and Recall"></a>Trade off between Precision and Recall</h5><p>High Precision or high Recall? Difficult choice isn’t it! A formula called F score makes it possible to assess the algorithm by a single numerical value:</p>
<script type="math/tex; mode=display">\text{F score} = 2* \ \frac{\text{Precision} * \text{Recall}}{\text{Precision} + \text{Recall}} \in [0, 1]</script><p>The higher the score, the better the result.</p>

      
    
    </div>
    
      <div class="post-tags syuanpi fadeInRightShort back-3">
      
        <a href="/tags/Machine-learning-Andrew-Ng/">Machine learning Andrew Ng</a>
      
      </div>
    
    
      

      
    
  </article>
  
    
  <nav class="article-page">
    
      <a href="/2019/07/18/ML-part4/" id="art-left" class="art-left">
        <span class="next-title">
          <i class="iconfont icon-back"></i>Notes for Coursera Machine Learning Week 7
        </span>
      </a>
    
    
      <a href="/2019/07/08/ML-part2/" id="art-right" class="art-right">
        <span class="prev-title">
          Notes for Coursera Machine Learning Week 4-5<i class="iconfont icon-enter"></i>
        </span>
      </a>
    
  </nav>


    
  <i id="com-switch" class="iconfont icon-more jumping-in long infinite" style="font-size:24px;display:block;text-align:center;transform:rotate(180deg);"></i>
  <div class="post-comments" id="post-comments" style="display: block;margin: auto 16px;">
    
    
    

    

  </div>



  
  
    
  
  <aside class="post-toc">
    <div class="title"><span>Index</span></div>
    <div class="toc-inner">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Week-6"><span class="toc-text">Week 6</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Evaluating-a-Hypothesis"><span class="toc-text">Evaluating a Hypothesis</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#The-test-set-error"><span class="toc-text">The test set error</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Cross-Validation"><span class="toc-text">Cross Validation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Diagnosing-Bias-vs-Variance"><span class="toc-text">Diagnosing Bias vs. Variance</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Regularization-and-Bias-Variance"><span class="toc-text">Regularization and Bias/Variance</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Learning-Curves"><span class="toc-text">Learning Curves</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Diagnosing-Neural-Networks"><span class="toc-text">Diagnosing Neural Networks</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Overall"><span class="toc-text">Overall</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Model-Complexity-Effects"><span class="toc-text">Model Complexity Effects:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Prioritizing-What-to-Work-On"><span class="toc-text">Prioritizing What to Work On</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Error-Analysis"><span class="toc-text">Error Analysis</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Precision-and-Recall"><span class="toc-text">Precision and Recall</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Trade-off-between-Precision-and-Recall"><span class="toc-text">Trade off between Precision and Recall</span></a></li></ol></li></ol></li></ol></li></ol>
    </div>
  </aside>



  


        </div>
      </main>
      <footer class="footer syuanpi fadeIn" id="footer">
  <hr>
  <div class="footer-wrapper">
    <div class="left">
      <div class="contact-icon">
    
    
    
    
    
    
    
    
    
        
            <a href="https://github.com/oalvay" class="iconfont icon-social-github" title="github"></a>
        
        
        
        
        
        
        
        
    
</div>

    </div>
    <div class="right">
      <div class="copyright">
    <div class="info">
        <span>&copy;</span>
        <span>2019 ~ 2019</span>
        <span>❤</span>
        <span>oalvay</span>
    </div>
    <div class="theme">
        <span>
            Powered by
            <a href="http://hexo.io/" target="_blank">Hexo </a>
        </span>
        <span>
            Theme
            <a href="https://github.com/ColMugX/hexo-theme-Nlvi"> Nlvi </a>
        </span>
    </div>
    
</div>

    </div>
  </div>
</footer>
    </div>
    <div class="tagcloud" id="tagcloud">
  <div class="tagcloud-taglist">
  
    <div class="tagcloud-tag">
      <button>主线剧情</button>
    </div>
  
    <div class="tagcloud-tag">
      <button>Machine learning Andrew Ng</button>
    </div>
  
    <div class="tagcloud-tag">
      <button>技多不压身</button>
    </div>
  
  </div>
  
    <div class="tagcloud-postlist active">
      <h2>主线剧情</h2>
      
        <div class="tagcloud-post">
          <a href="/2019/08/06/ML-conclusion/">
            <time class="tagcloud-posttime">2019 / 08 / 06</time>
            <span>回首，小结与岔路口</span>
          </a>
        </div>
      
        <div class="tagcloud-post">
          <a href="/2019/06/17/hello-world/">
            <time class="tagcloud-posttime">2019 / 06 / 17</time>
            <span>Hello Hexo</span>
          </a>
        </div>
      
    </div>
  
    <div class="tagcloud-postlist ">
      <h2>Machine learning Andrew Ng</h2>
      
        <div class="tagcloud-post">
          <a href="/2019/07/18/ML-part4/">
            <time class="tagcloud-posttime">2019 / 07 / 18</time>
            <span>Notes for Coursera Machine Learning Week 7</span>
          </a>
        </div>
      
        <div class="tagcloud-post">
          <a href="/2019/07/21/ML-part5/">
            <time class="tagcloud-posttime">2019 / 07 / 21</time>
            <span>Notes for Coursera Machine Learning Week 8</span>
          </a>
        </div>
      
        <div class="tagcloud-post">
          <a href="/2019/07/16/ML-part3/">
            <time class="tagcloud-posttime">2019 / 07 / 16</time>
            <span>Notes for Coursera Machine Learning Week 6</span>
          </a>
        </div>
      
        <div class="tagcloud-post">
          <a href="/2019/07/02/ML-part1/">
            <time class="tagcloud-posttime">2019 / 07 / 02</time>
            <span>Notes for Coursera Machine Learning Week 1-3</span>
          </a>
        </div>
      
        <div class="tagcloud-post">
          <a href="/2019/07/08/ML-part2/">
            <time class="tagcloud-posttime">2019 / 07 / 08</time>
            <span>Notes for Coursera Machine Learning Week 4-5</span>
          </a>
        </div>
      
        <div class="tagcloud-post">
          <a href="/2019/07/25/ML-part6/">
            <time class="tagcloud-posttime">2019 / 07 / 25</time>
            <span>Notes for Coursera Machine Learning Week 9</span>
          </a>
        </div>
      
        <div class="tagcloud-post">
          <a href="/2019/08/04/ML-part7/">
            <time class="tagcloud-posttime">2019 / 08 / 04</time>
            <span>Notes for Coursera Machine Learning Week 10-11</span>
          </a>
        </div>
      
    </div>
  
    <div class="tagcloud-postlist ">
      <h2>技多不压身</h2>
      
        <div class="tagcloud-post">
          <a href="/2019/06/17/hexo-github-搭建博客小记/">
            <time class="tagcloud-posttime">2019 / 06 / 17</time>
            <span>hexo+github 搭建博客小记</span>
          </a>
        </div>
      
    </div>
  
</div>

  </div>
  <div class="backtop syuanpi melt toTop" id="backtop">
    <i class="iconfont icon-up"></i>
    <span style="text-align:center;font-family:Georgia;"><span style="font-family:Georgia;" id="scrollpercent">1</span>%</span>
</div>


<script src="/script/lib/jquery/jquery-3.2.1.min.js"></script>


  <script src="/script/lib/lightbox/js/lightbox.min.js"></script>



  <script src="https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/x-mathjax-config;executed=true">MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"], ["\\(","\\)"]]}});</script>







  <script src="/script/scheme/banderole.js"></script>


<script src="/script/bootstarp.js"></script>


<script>
if (nlviconfig.theme.toc) {
  setTimeout(function() {
    if (nlviconfig.theme.scheme === 'balance') {
      $("#header").addClass("show_toc");
    } else if (nlviconfig.theme.scheme === 'banderole') {
      $(".container-inner").addClass("has_toc");
      $(".post-toc .title").addClass("show");
      $(".toc-inner").addClass("show");
    }
  }, 1000);
}
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



</body>
</html>
