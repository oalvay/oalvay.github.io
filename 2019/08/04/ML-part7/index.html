<!DOCTYPE html>
<html lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="browsermode" content="application">
<meta name="apple-touch-fullscreen" content="yes">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="Oalvay's Blog">
<meta name="apple-mobile-web-app-status-bar-style" content="default">
<meta name="msapplication-navbutton-color" content="#666666">
<meta name="format-detection" content="telephone=no">





  <meta name="keywords" content="Machine learning Andrew Ng, nlvi">


<link rel="apple-touch-startup-image" media="(device-width: 375px)" href="assets/apple-launch-1125x2436.png">
<link rel="apple-touch-startup-image" media="(orientation: landscape)" href="assets/apple-touch-startup-image-2048x1496.png">
<link rel="stylesheet" href="/style/style.css">
<script>
  var nlviconfig = {
    title: "Oalvay's Blog",
    author: "oalvay",
    baseUrl: "/",
    theme: {
      scheme: "banderole",
      lightbox: true,
      animate: true,
      search: false,
      friends: false,
      reward: false,
      pjax: false,
      lazy: false,
      toc: true
    }
  }
</script>




    <link rel="stylesheet" href="/script/lib/lightbox/css/lightbox.min.css">




    <link rel="stylesheet" href="/syuanpi/syuanpi.min.css">













  <title> Notes for Coursera Machine Learning Week 10-11 · Oalvay's Blog </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container" style="display:none;">
    <header class="header" id="header">
  <div class="header-wrapper">
    <div class="logo">
  <div class="logo-inner syuanpi tvIn">
    <h1><a href="/">Oalvay's Blog</a></h1>
    
  </div>
</div>

    <nav class="main-nav">
  
  <ul class="main-nav-list syuanpi tvIn">
  
  
  
    
  
    <li class="menu-item">
      <a href="/about" id="about">
        <span class="base-name">
          
            ABOUT
          
        </span>
      </a>
    </li>
  
  
    
  
    <li class="menu-item">
      <a href="javascript:;" id="tags">
        <span class="base-name">
          
            TAGS
          
        </span>
      </a>
    </li>
  
  
    
  
    <li class="menu-item">
      <a href="/archives" id="archives">
        <span class="base-name">
          
            ARCHIVES
          
        </span>
      </a>
    </li>
  
  
  </ul>
  
</nav>

  </div>
</header>
<div class="mobile-header" id="mobile-header">
  <div class="mobile-header-nav">
    <div class="mobile-header-item" id="mobile-left">
      <div class="header-menu-item">
        <div class="header-menu-line"></div>
      </div>
    </div>
    <h1 class="mobile-header-title">
      <a href="/">Oalvay's Blog</a>
    </h1>
    <div class="mobile-header-item"></div>
  </div>
  <div class="mobile-header-body">
    <ul class="mobile-header-list">
      
        <li class="mobile-nav-item syuanpi fadeInRightShort back-0">
          <a href="/about">
            
              ABOUT
            
          </a>
        </li>
      
        <li class="mobile-nav-item syuanpi fadeInRightShort back-1">
          <a href="javascript:;" id="mobile-tags">
            
              TAGS
            
          </a>
        </li>
      
        <li class="mobile-nav-item syuanpi fadeInRightShort back-2">
          <a href="/archives">
            
              ARCHIVES
            
          </a>
        </li>
      
    </ul>
  </div>
</div>



    <div class="container-inner">
      <main class="main" id="main">
        <div class="main-wrapper">
          
    
  
  <article class="
  post
   is_post 
  ">
    <header class="post-header">
      <div class="post-time syuanpi fadeInRightShort back-1">
        <div class="post-time-wrapper">
          <time>2019-08-04</time>
          
        </div>
      </div>
      <h2 class="post-title syuanpi fadeInRightShort back-2">
        
          Notes for Coursera Machine Learning Week 10-11
        
      </h2>
    </header>
    <div class="post-content syuanpi fadeInRightShort back-3">
      
        <p>Contents: More types of gradient descent, online learning, pipeline and more</p>
<a id="more"></a>
<h2 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h2><p><strong>Batch</strong> Gradient descent: using the whole dataset in every iteration</p>
<h3 id="Stochastic-gradient-descent"><a href="#Stochastic-gradient-descent" class="headerlink" title="Stochastic gradient descent"></a>Stochastic gradient descent</h3><p>Unlike Batch Gradient descent, this approach uses only one example in one iteration. Here is how it works:</p>
<ol>
<li>Randomly shuffle (reorder) training examples</li>
<li>Repeat one or several times {<br>for i = 1,…,m<br>&emsp; &emsp; &emsp; <script type="math/tex">\theta_j := \theta_j - \alpha (h_\theta(x^{(i)}) - y^{(i)}) x^{(i)}_j</script> for j = 1,…,n<br>}</li>
<li>Everything else is the same as in Batch gradient descent</li>
</ol>
<p>Stochastic gradient descent is often not going to reach global minimum, but instead ends up wandering around it’s neighboring area. This is not a big problem in practice as long as it is close enough.</p>
<h3 id="Mini-batch-gradient-descent"><a href="#Mini-batch-gradient-descent" class="headerlink" title="Mini-batch gradient descent"></a>Mini-batch gradient descent</h3><p>So far we have seen two types of gradient descent, here we introduce the final one by comparing them:</p>
<ul>
<li>Batch gradient descent: Use all m examples in each iteration  </li>
<li>Stochastic gradient descent: Use 1 example in each iteration </li>
<li>Mini-batch gradient descent: Use b examples in each iteration</li>
</ul>
<p>This approach works as following:</p>
<ol>
<li>Let b = 10, say.</li>
<li>Randomly shuffle (reorder) training examples</li>
<li>Repeat one or several times {<br>for i = 1,11,21,…,m-9<br>&emsp; &emsp; &emsp; <script type="math/tex">\theta_j := \theta_j - \frac{\alpha}{10} \sum^{10}_{k=1} (h_\theta(x^{(k)}) - y^{(k)}) x^{(k)}_j</script> for j = 1,…,n<br>}</li>
<li>Everything else is the same as in Batch gradient descent</li>
</ol>
<p>Mini-batch gradient descent allows us to perform vertorization, which leads to higher efficiency such that it may work even faster than the Stochastic one.</p>
<h3 id="Gradient-descent-convergence"><a href="#Gradient-descent-convergence" class="headerlink" title="Gradient descent convergence"></a>Gradient descent convergence</h3><p>With batch gradient descent, we plot cost at each iteration against no. iterations to check convergence. For the other two types of gradient descent, similar idea is applied.</p>
<p>We take stochastic gradient descent as example. Here is the cost function for it:</p>
<script type="math/tex; mode=display">cost (\theta, (x^{(i)}, y^{(i)})) = \frac{1}{2} (h_\theta(x^{(i)}) - y^{(i)})^2</script><p>It’s certainly not a good idea to plot it against no. iteration straight away. So instead, we use the cost <strong>averaged</strong> over the last i examples:</p>
<script type="math/tex; mode=display">\frac{1}{i} \sum_{k=1}^icost (\theta, (x^{(k)}, y^{(k)}))</script><p>This gives a better view of how the algorithm has performed on last i examples, so we should be able to check convergence.</p>
<h4 id="Learning-rate"><a href="#Learning-rate" class="headerlink" title="Learning rate"></a>Learning rate</h4><p>Normally, like cost of batch gradient descent, the averaged cost going downwards gently as no. iteration increases, but learning rate can have effects on it:</p>
<p><img src="http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[20].png" size></p>
<p>We see that with smaller learning rate, the algorithm can end up with slightly better result, but note that smaller learning rate would slow the speed of convergence. Moreover, in the case that the averaged cost is going upwards (indicates divergence), making learning rate smaller would also help.</p>
<p>On the other hand, if we want θ to converge, then rathen than holding learning rate constant, we can slowly decrease α over time to do so:</p>
<script type="math/tex; mode=display">\alpha = \frac{c_1}{c_2 + \text{no. interation}}</script><p>where <script type="math/tex">c_1, c_2</script> is the additional parameters we need to mannually manipulate with.</p>
<h2 id="Online-learning"><a href="#Online-learning" class="headerlink" title="Online learning"></a>Online learning</h2><p>Online learning algorithms has a property that each example is processed only once, such that it usually best suited to problems were we have a continuous/non-stop stream of data that we want to learn from.</p>
<p>The approach works like this:</p>
<p>Repeat <strong>forever</strong> {<br>get information from new user (x, y)<br>Update θ using (x, y):<br>  &emsp; &emsp; &emsp; <script type="math/tex">\theta_j := \theta_j - \alpha (h_\theta(x) - y) x_j</script> for j = 1,…,n<br>}</p>
<p>So the algorithm keep updating θ whenever new x comes in. It’s easy to imagine the possible application scenarios: Choosing special offers to show user; customized selection of news articles; product recommendation. The course gives a detailed example to show how it can be applied:</p>
<p>Product search (learning to search)</p>
<ul>
<li>User searches for “Android phone 1080p camera” </li>
<li>Have 100 phones in store. Will return 10 results.</li>
<li>x = features of phone, how many words in user query match name of phone, how many words in query match description of phone, etc.</li>
<li>y = 1 if user clicks on link. y = 0 otherwise.</li>
<li>Learn <script type="math/tex">p(y = 1\vert x;\theta)</script> (also called the predicted <strong>click through rate (CTR)</strong>)</li>
<li>Use to show user the 10 phones they’re most likely to click on.</li>
</ul>
<p>One advantage of online learning if the function we’re modeling changes over time (such as if we are modeling the probability (<script type="math/tex">p(y\vert x;\theta)</script>) of users clicking on different URLs, and user tastes/preferences/ are changing over time), the online learning algorithm will automatically adapt to these changes.</p>
<h2 id="Map-reduce-and-data-parallelism"><a href="#Map-reduce-and-data-parallelism" class="headerlink" title="Map-reduce and data parallelism"></a>Map-reduce and data parallelism</h2><p>No fancy things in this topic, so I will only write a short introdction:</p>
<p>Map-reduce basically means use more than one computer to calculate the summation part, so for example if we sum over 400 examples, just let 4 computers to do 100 of them respectively. Similarly, data parallelism just means apply multithreading stuff to the calculation.</p>
<p>I know you are thinking what a lazy guy I am, so here is a <a href="http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning.html" target="_blank">detailed version</a> with Q&amp;A related to big data that I strongly recommended.</p>
<h1 id="Week-11"><a href="#Week-11" class="headerlink" title="Week 11"></a>Week 11</h1><h2 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h2><p>Having a complex machine learning problem is common in the real world, where there is not a easy way to solve it. So it is usually helpful to divide the problem into several components such that we can then work on them part by part. Such technique of building a system with many stages / components, several of which may use machine learning is called a <strong>pipeline</strong>.</p>
<p>The course gives an example of Photo OCR(photo optical character recognition), where it was divided into four components:</p>
<ul>
<li>Image</li>
<li>Text detection</li>
<li>Character segmentation </li>
<li>Character Recognition</li>
</ul>
<p>Text detection is the part where the course spent most of time with, the ideas of <strong>sliding windows</strong> and <strong>expansion algorithm</strong> were introduced.</p>
<h2 id="Artificial-data-synthesis"><a href="#Artificial-data-synthesis" class="headerlink" title="Artificial data synthesis"></a>Artificial data synthesis</h2><p>This technique is one way of getting a large amount of training examples, where we are trying to make new examples artificially by simulating / modifying the original ones. E.g. Character Recognition:</p>
<p><img src="http://1.bp.blogspot.com/-JsKbp7xmBW0/T-8XM-VtapI/AAAAAAAAAWk/fUDNXcm6I4E/w1200-h630-p-k-no-nu/syn_data.png" size></p>
<p>Based on the graph on the left, we can manually create new examples by using different fonts, paste these characters in random backgrounds and apply blurring/distortion filters, which shows on the right.</p>
<p>Another way is to modify the original example by, for example, warping it:</p>
<p><img src="http://www.holehouse.org/mlclass/18_Application_Example_OCR_files/Image%20[14].png" size></p>
<h2 id="Ceiling-analysis"><a href="#Ceiling-analysis" class="headerlink" title="Ceiling analysis"></a>Ceiling analysis</h2><p>Ceiling analysis is a way to guide you which part of the pipeline to work on next. It can help indicate that certain components of a system might not be worth a significant amount of work improving, because even if it had perfect performance its impact on the overall system may be small. Therefore it helps us decide on allocation of resources in terms of which component in a machine learning pipeline to spend more effort on.<br>We take Photo OCR as example again:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Component</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Overall performance</td>
<td>70%</td>
</tr>
<tr>
<td>Text detection</td>
<td>85%</td>
</tr>
<tr>
<td>Character segmentation</td>
<td>89%</td>
</tr>
<tr>
<td>Character Recognition</td>
<td>100%</td>
</tr>
</tbody>
</table>
</div>
<p>It might be unclear what is going on in the above, so I take the text detection part to explain: after using correct answer straight away instead of machine learning algorithm, the accuracy increases from 70% to 85%.</p>
<p>So there is a large gain in performance possible in improving the text detection and character recognition part, and we should not spend too much time on improving character segmentation.</p>
<p>Note that performing the ceiling analysis shown here requires that we have ground-truth labels for the text detection, character segmentation and the character recognition systems.</p>

      
    
    </div>
    
      <div class="post-tags syuanpi fadeInRightShort back-3">
      
        <a href="/tags/Machine-learning-Andrew-Ng/">Machine learning Andrew Ng</a>
      
      </div>
    
    
      

      
    
  </article>
  
    
  <nav class="article-page">
    
      <a href="/2019/08/06/ML-conclusion/" id="art-left" class="art-left">
        <span class="next-title">
          <i class="iconfont icon-back"></i>回首，小结与岔路口
        </span>
      </a>
    
    
      <a href="/2019/07/25/ML-part6/" id="art-right" class="art-right">
        <span class="prev-title">
          Notes for Coursera Machine Learning Week 9<i class="iconfont icon-enter"></i>
        </span>
      </a>
    
  </nav>


    
  <i id="com-switch" class="iconfont icon-more jumping-in long infinite" style="font-size:24px;display:block;text-align:center;transform:rotate(180deg);"></i>
  <div class="post-comments" id="post-comments" style="display: block;margin: auto 16px;">
    
    
    

    

  </div>



  
  
    
  
  <aside class="post-toc">
    <div class="title"><span>Index</span></div>
    <div class="toc-inner">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-descent"><span class="toc-text">Gradient descent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-gradient-descent"><span class="toc-text">Stochastic gradient descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-batch-gradient-descent"><span class="toc-text">Mini-batch gradient descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-descent-convergence"><span class="toc-text">Gradient descent convergence</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Learning-rate"><span class="toc-text">Learning rate</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Online-learning"><span class="toc-text">Online learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Map-reduce-and-data-parallelism"><span class="toc-text">Map-reduce and data parallelism</span></a></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#Week-11"><span class="toc-text">Week 11</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Pipeline"><span class="toc-text">Pipeline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Artificial-data-synthesis"><span class="toc-text">Artificial data synthesis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ceiling-analysis"><span class="toc-text">Ceiling analysis</span></a></li></ol>
    </li></div>
  </aside>



  


        </div>
      </main>
      <footer class="footer syuanpi fadeIn" id="footer">
  <hr>
  <div class="footer-wrapper">
    <div class="left">
      <div class="contact-icon">
    
    
    
    
    
    
    
    
    
        
            <a href="https://github.com/oalvay" class="iconfont icon-social-github" title="github"></a>
        
        
        
        
        
        
        
        
    
</div>

    </div>
    <div class="right">
      <div class="copyright">
    <div class="info">
        <span>&copy;</span>
        <span>2019 ~ 2019</span>
        <span>❤</span>
        <span>oalvay</span>
    </div>
    <div class="theme">
        <span>
            Powered by
            <a href="http://hexo.io/" target="_blank">Hexo </a>
        </span>
        <span>
            Theme
            <a href="https://github.com/ColMugX/hexo-theme-Nlvi"> Nlvi </a>
        </span>
    </div>
    
</div>

    </div>
  </div>
</footer>
    </div>
    <div class="tagcloud" id="tagcloud">
  <div class="tagcloud-taglist">
  
    <div class="tagcloud-tag">
      <button>主线剧情</button>
    </div>
  
    <div class="tagcloud-tag">
      <button>Machine learning Andrew Ng</button>
    </div>
  
    <div class="tagcloud-tag">
      <button>技多不压身</button>
    </div>
  
  </div>
  
    <div class="tagcloud-postlist active">
      <h2>主线剧情</h2>
      
        <div class="tagcloud-post">
          <a href="/2019/08/06/ML-conclusion/">
            <time class="tagcloud-posttime">2019 / 08 / 06</time>
            <span>回首，小结与岔路口</span>
          </a>
        </div>
      
        <div class="tagcloud-post">
          <a href="/2019/06/17/hello-world/">
            <time class="tagcloud-posttime">2019 / 06 / 17</time>
            <span>Hello Hexo</span>
          </a>
        </div>
      
    </div>
  
    <div class="tagcloud-postlist ">
      <h2>Machine learning Andrew Ng</h2>
      
        <div class="tagcloud-post">
          <a href="/2019/07/18/ML-part4/">
            <time class="tagcloud-posttime">2019 / 07 / 18</time>
            <span>Notes for Coursera Machine Learning Week 7</span>
          </a>
        </div>
      
        <div class="tagcloud-post">
          <a href="/2019/07/21/ML-part5/">
            <time class="tagcloud-posttime">2019 / 07 / 21</time>
            <span>Notes for Coursera Machine Learning Week 8</span>
          </a>
        </div>
      
        <div class="tagcloud-post">
          <a href="/2019/07/16/ML-part3/">
            <time class="tagcloud-posttime">2019 / 07 / 16</time>
            <span>Notes for Coursera Machine Learning Week 6</span>
          </a>
        </div>
      
        <div class="tagcloud-post">
          <a href="/2019/07/02/ML-part1/">
            <time class="tagcloud-posttime">2019 / 07 / 02</time>
            <span>Notes for Coursera Machine Learning Week 1-3</span>
          </a>
        </div>
      
        <div class="tagcloud-post">
          <a href="/2019/07/08/ML-part2/">
            <time class="tagcloud-posttime">2019 / 07 / 08</time>
            <span>Notes for Coursera Machine Learning Week 4-5</span>
          </a>
        </div>
      
        <div class="tagcloud-post">
          <a href="/2019/07/25/ML-part6/">
            <time class="tagcloud-posttime">2019 / 07 / 25</time>
            <span>Notes for Coursera Machine Learning Week 9</span>
          </a>
        </div>
      
        <div class="tagcloud-post">
          <a href="/2019/08/04/ML-part7/">
            <time class="tagcloud-posttime">2019 / 08 / 04</time>
            <span>Notes for Coursera Machine Learning Week 10-11</span>
          </a>
        </div>
      
    </div>
  
    <div class="tagcloud-postlist ">
      <h2>技多不压身</h2>
      
        <div class="tagcloud-post">
          <a href="/2019/06/17/hexo-github-搭建博客小记/">
            <time class="tagcloud-posttime">2019 / 06 / 17</time>
            <span>hexo+github 搭建博客小记</span>
          </a>
        </div>
      
    </div>
  
</div>

  </div>
  <div class="backtop syuanpi melt toTop" id="backtop">
    <i class="iconfont icon-up"></i>
    <span style="text-align:center;font-family:Georgia;"><span style="font-family:Georgia;" id="scrollpercent">1</span>%</span>
</div>


<script src="/script/lib/jquery/jquery-3.2.1.min.js"></script>


  <script src="/script/lib/lightbox/js/lightbox.min.js"></script>



  <script src="https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/x-mathjax-config;executed=true">MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"], ["\\(","\\)"]]}});</script>







  <script src="/script/scheme/banderole.js"></script>


<script src="/script/bootstarp.js"></script>


<script>
if (nlviconfig.theme.toc) {
  setTimeout(function() {
    if (nlviconfig.theme.scheme === 'balance') {
      $("#header").addClass("show_toc");
    } else if (nlviconfig.theme.scheme === 'banderole') {
      $(".container-inner").addClass("has_toc");
      $(".post-toc .title").addClass("show");
      $(".toc-inner").addClass("show");
    }
  }, 1000);
}
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



</body>
</html>
