<!DOCTYPE html>
<html lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="browsermode" content="application">
<meta name="apple-touch-fullscreen" content="yes">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="Oalvay's Blog">
<meta name="apple-mobile-web-app-status-bar-style" content="default">
<meta name="msapplication-navbutton-color" content="#666666">
<meta name="format-detection" content="telephone=no">






<link rel="apple-touch-startup-image" media="(device-width: 375px)" href="assets/apple-launch-1125x2436.png">
<link rel="apple-touch-startup-image" media="(orientation: landscape)" href="assets/apple-touch-startup-image-2048x1496.png">
<link rel="stylesheet" href="/style/style.css">
<script>
  var nlviconfig = {
    title: "Oalvay's Blog",
    author: "oalvay",
    baseUrl: "/",
    theme: {
      scheme: "banderole",
      lightbox: true,
      animate: true,
      search: false,
      friends: false,
      reward: false,
      pjax: false,
      lazy: false,
      toc: true
    }
  }
</script>




    <link rel="stylesheet" href="/script/lib/lightbox/css/lightbox.min.css">




    <link rel="stylesheet" href="/syuanpi/syuanpi.min.css">













  <title>
  
    Archives · Oalvay's Blog
  
</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container" style="display:none;">
    <header class="header" id="header">
  <div class="header-wrapper">
    <div class="logo">
  <div class="logo-inner syuanpi tvIn">
    <h1><a href="/">Oalvay's Blog</a></h1>
    
  </div>
</div>

    <nav class="main-nav">
  
  <ul class="main-nav-list syuanpi tvIn">
  
  
  
    
  
    <li class="menu-item">
      <a href="/about" id="about">
        <span class="base-name">
          
            ABOUT
          
        </span>
      </a>
    </li>
  
  
    
  
    <li class="menu-item">
      <a href="javascript:;" id="tags">
        <span class="base-name">
          
            TAGS
          
        </span>
      </a>
    </li>
  
  
    
  
    <li class="menu-item">
      <a href="/archives" id="archives">
        <span class="base-name">
          
            ARCHIVES
          
        </span>
      </a>
    </li>
  
  
  </ul>
  
</nav>

  </div>
</header>
<div class="mobile-header" id="mobile-header">
  <div class="mobile-header-nav">
    <div class="mobile-header-item" id="mobile-left">
      <div class="header-menu-item">
        <div class="header-menu-line"></div>
      </div>
    </div>
    <h1 class="mobile-header-title">
      <a href="/">Oalvay's Blog</a>
    </h1>
    <div class="mobile-header-item"></div>
  </div>
  <div class="mobile-header-body">
    <ul class="mobile-header-list">
      
        <li class="mobile-nav-item syuanpi fadeInRightShort back-0">
          <a href="/about">
            
              ABOUT
            
          </a>
        </li>
      
        <li class="mobile-nav-item syuanpi fadeInRightShort back-1">
          <a href="javascript:;" id="mobile-tags">
            
              TAGS
            
          </a>
        </li>
      
        <li class="mobile-nav-item syuanpi fadeInRightShort back-2">
          <a href="/archives">
            
              ARCHIVES
            
          </a>
        </li>
      
    </ul>
  </div>
</div>



    <div class="container-inner">
      <main class="main" id="main">
        <div class="main-wrapper">
          
    
  <div class="archive syuanpi fadeInRightShort" id="archive">
    
      <aside class="archive-title">Total</aside>
      <span class="archive-num">10 Posts In Total</span>
    
      
      
        
          
          
            
            
            <div class="archive-group">
            <time class="archive-year"> 2019 </time>
            <div class="archive-list">
          
          
        <div class="archive-post">
          <div class="archive-post-content">
            
              <p>Conservatives manifesto — 保守党宣言<br>pledges, commitment — 承诺<br>Dispatch Boxes — 调度箱，用来运输机密公务文件<br>Uxbridge and South Ruislip — Boris的议会选区，被Corbin用来代指Boris</p>
<p>Corbin 疯狂强调 Boris 的首相职位并非由选举而来</p>

            
          </div>
          <div class="archive-post-info">
            <time class="archive-post-time">
              2019-07-25
            </time>
            <a class="archive-post-link" href="/2019/07/25/PMQs-24-jul-2019/">
              PMQs-24-jul-2019
            </a>
          </div>
        </div>
      
        
          
          
          
        <div class="archive-post">
          <div class="archive-post-content">
            
              <h2 id="Anomaly-detection"><a href="#Anomaly-detection" class="headerlink" title="Anomaly detection"></a>Anomaly detection</h2><p>In a nutshell, it’s about using training set to obtain maximum likelyhood estimation(MLE) of parameters of <strong>Normal distribution</strong>, then use it to predict the probability of new examples being anomalies.</p>
<ul>
<li>Use training sets to calculate <strong>MLE</strong> of parameters <script type="math/tex">\mu_1,...,\mu_n, \sigma^2_1,..., \sigma2_n</script>:</li>
</ul>
<script type="math/tex; mode=display">\mu_j = \frac{1}{m} \sum_{i=1}^{m} x^{(i)}_j</script><script type="math/tex; mode=display">\sigma_j^2 = \frac{1}{m} \sum_{i=1}^{m} (x^{(i)}_j - \mu_j)^2</script><ul>
<li>compute p(x) for new example <script type="math/tex">x \in \mathbb{R}^n</script>:</li>
</ul>
<script type="math/tex; mode=display">p(x) = \prod_{i=1}^n p(x_j; \mu_j, \sigma_j^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma_j}\exp\left(-\frac{(x-\mu_j)^2}{2\sigma_j^2}\right)</script><ul>
<li>preidct y based on p(x) with ϵ</li>
</ul>
<script type="math/tex; mode=display">y = \left\{\begin{array}{ll} 1 & \mbox{if } p(x) \leq \epsilon\\ 0 & \mbox{if } p(x) > \epsilon\end{array}\right.</script><p>Note that we assume all features are all <strong>independent</strong>, if any correlation exists amongest them, then either create new features manually (e.g. <script type="math/tex">x_3 = \frac{x_1}{x_2}</script>) to detect anomalies instead of the original ones (<script type="math/tex">x_1, x_2</script>), or use the <strong>multivariate Normal distribution</strong> (as the parameters of corvariace matrix ∑ generates potential relations automatically).</p>
<h3 id="Evaluation-of-performace"><a href="#Evaluation-of-performace" class="headerlink" title="Evaluation of performace"></a>Evaluation of performace</h3><p>Classification accuracy is not a good way to measure the algorithm’s performance, because of skewed classes (so an algorithm that always predicts y = 0 will have high accuracy). So people usually prefer F-Score, Precision &amp; Recall or some other methods.</p>
<h3 id="Split-between-train-amp-cv-sets-and-Choosing-ϵ"><a href="#Split-between-train-amp-cv-sets-and-Choosing-ϵ" class="headerlink" title="Split between train &amp; cv sets and Choosing ϵ"></a>Split between train &amp; cv sets and Choosing ϵ</h3><ul>
<li><p>We are not able to use anomalies for training (get MLEs) so shall not include anu of them in the training sets. Instead use these anomalies in cv and test sets with a 50:50 split would be a nice choice. For instance, if we have 10000 normal and 20 abnormal examples, use 6000 of the normal ones for training, 2000 of normal ones and 10 of abnormal ones for cross validation and the rest for test.</p>
</li>
<li><p>The course did not introduce any method of choosing ϵ automatically, so we have to choose it manually (e.g. base on the performance on cross validation set).</p>
</li>
</ul>
<h3 id="Anomaly-detection-vs-Supervised-learning"><a href="#Anomaly-detection-vs-Supervised-learning" class="headerlink" title="Anomaly detection vs. Supervised learning"></a>Anomaly detection vs. Supervised learning</h3><p>The usage of anomaly detection is kind of like classification isn’t it? You may wondering why not use supervised learning such as logistic regression instead, so the below explained how supervised learning would perform in two cases:</p>
<ul>
<li>skewed data —- Hard for supervised algorithm to learn. As there exists potentially many different types of anormalies, so future anomalies may look nothing like any of the anomalous examples we’ve seen so far.</li>
<li>balanced data —-  Enough positive examples for algorithm to get a sense of what positive examples are like.</li>
</ul>
<p>So use supervised learning algorithm when the number of positive and negative examples are both large, otherwise anomaly detection would be prefered.</p>
<p>The course gives some application scenario which can help you understand better:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Anomaly detection</th>
<th>Supervised learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Fraud detection</td>
<td>Email spam classification (easy to get large number of spams)</td>
</tr>
<tr>
<td>Manufacturing (e.g. aircraft engines)</td>
<td>Weather prediction (sunny/ rainy/etc).</td>
</tr>
<tr>
<td>Monitoring machines in a data center</td>
<td>Cancer classification</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Non-gaussian-features"><a href="#Non-gaussian-features" class="headerlink" title="Non-gaussian features"></a>Non-gaussian features</h3><p>If the data of a feature not bell-shaped curve, using one-to-one transformation (e.g. natural log for positively skewed data) might be a good idea.</p>
<h2 id="Recommender-System"><a href="#Recommender-System" class="headerlink" title="Recommender System"></a>Recommender System</h2><h3 id="Content-Based-Recommendation"><a href="#Content-Based-Recommendation" class="headerlink" title="Content Based Recommendation"></a>Content Based Recommendation</h3><p>For this approachm, we assume that the features of contents are available to us. That means, for instance, we know how much romance or action content is in a movie, such that we are able to use these features to make predictions. The course gave the example below:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Movie</th>
<th>Alice (1)</th>
<th>Bob (2)</th>
<th>Carol (3)</th>
<th>David (4)</th>
<th>(romance)</th>
<th>(action)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Love at last</td>
<td>5</td>
<td>5</td>
<td>0</td>
<td>0</td>
<td>0.9</td>
<td>0</td>
</tr>
<tr>
<td>Romance forever</td>
<td>5</td>
<td>?</td>
<td>?</td>
<td>0</td>
<td>1.0</td>
<td>0.01</td>
</tr>
<tr>
<td>Cute puppies of love</td>
<td>?</td>
<td>4</td>
<td>0</td>
<td>?</td>
<td>0.99</td>
<td>0</td>
</tr>
<tr>
<td>Nonstop car chases</td>
<td>0</td>
<td>0</td>
<td>5</td>
<td>4</td>
<td>0.1</td>
<td>1.0</td>
</tr>
<tr>
<td>Swords vs. karate</td>
<td>0</td>
<td>0</td>
<td>5</td>
<td>?</td>
<td>0</td>
<td>0.9</td>
</tr>
</tbody>
</table>
</div>
<p>We now define some notations for later problem formulation:</p>
<script type="math/tex; mode=display">n_u$$ = no. users  
$$n_m$$ = no. movies  
$$r(i, j) = 1$$ if user j has rated movie i  
$$y^{(i, j)}$$ = rating given by user j to movie i (define only if $$r(i, j) = 1$$)  
$$x^{(i)}$$ = feature vector for movie i  
$$\theta^{(j)}$$: For each user j, learn a parameter $$\theta^{(j)} \in \mathbb{R}^3$$ to predict how this user would rate movie i with $$(\theta^{(j)})^T x^{(i)} $$ stars.  

For example, with the parameter $$\theta^{(1)} = \begin{bmatrix}0\\5\\0\end{bmatrix}$$, we predict Alice would rate *Cute puppies of love* a $$(\theta^{(1)})^T x^{(3)} = \begin{bmatrix}1 \ \ 0.99 \ \ 0\end{bmatrix} \begin{bmatrix}0\\5\\0\end{bmatrix} = 4.95$$ stars.


### Optimization objective

To learn a single parameter $$\theta^{(j)}$$ for user j:

$$\min_{\theta^{(j)}} \frac{1}{2} \sum_{i: r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2} \sum^n_{k=1} (\theta^{(j)}_k)^2</script><p>So to learn all parameters <script type="math/tex">\theta^{(1)}, \theta^{(2)}, ..., \theta^{(n_u)}</script> simultaneously, we simply sum cost functions over all users :</p>
<script type="math/tex; mode=display">\min_{\theta^{(1)}, \theta^{(2)}, ..., \theta^{(n_u)}} \frac{1}{2}\color{Blue}{
\sum^{n_u}_{j=1} \sum_{i: r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i, j)})^2 } + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum^n_{k=1} (\theta^{(j)}_k)^2</script><h4 id="Gradient-Descend-update"><a href="#Gradient-Descend-update" class="headerlink" title="Gradient Descend update"></a>Gradient Descend update</h4><script type="math/tex; mode=display">\theta^{(j)}_k := 
\left\{\begin{array}{ll}
 \theta^{(j)}_k - \alpha \sum_{i: r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) x^{(k)} \ \ & \mbox{for k = 0}\\
\theta^{(j)}_k - \alpha \left ( \sum_{i: r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) x^{(k)} + \lambda \theta^{(j)}_k \right )\ \ & \mbox{for k ≠ 0} 
\end{array}\right.</script><p>for <script type="math/tex">k = 1, ..., n_u</script>. </p>
<h3 id="Collaborative-Filtering"><a href="#Collaborative-Filtering" class="headerlink" title="Collaborative Filtering"></a>Collaborative Filtering</h3><p>Now the roles are swaped. Given the parameters θ’s, the algorithm is able to predict x’s as the following: </p>
<script type="math/tex; mode=display">\displaystyle \min_{x^{(1)},\dots,x^{(n_m)}} \frac{1}{2} \color{Blue}{
\sum_{i=1}^{n_m}\sum_{j:r(i,j)=1}\left((\theta^{(j)})^T x^{(i)}-y^{(i,j)}\right)^2 } + \frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n(x_k^{(i)})^2</script><p>With Gradient Descend update:</p>
<script type="math/tex; mode=display">x^{(i)}_k := 
\left\{\begin{array}{ll}
 x^{(i)}_k - \alpha \sum_{j: r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) \ \theta^{(j)} \ \ & \mbox{for k = 0}\\
x^{(i)}_k - \alpha \left ( \sum_{j: r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) \ \theta^{(j)} + \lambda x^{(i)}_k \right )\ \ & \mbox{for k ≠ 0} 
\end{array}\right.</script><p>Therefore, given <script type="math/tex">x^{(1)},\dots,x^{(n_m)}</script>, we can estimate <script type="math/tex">\theta^{(1)},..., \theta^{(n_u)}</script>.<br>And given <script type="math/tex">\theta^{(1)},..., \theta^{(n_u)}</script>, we can estimate <script type="math/tex">x^{(1)},\dots,x^{(n_m)}</script>.  </p>
<p>This algorithm that we are going to use is so-called <strong>Collaborative Filtering</strong>, where θ’s and x’s are estimated to estimate another.<br>You might have noticed the blue-highlighted parts in the cost functions above are actually the same. In fact, rather than update one follow by another, we are able to come up with a more efficent and elegant way to train the algorithm:</p>
<script type="math/tex; mode=display">\min_{x^{(1)},\dots,x^{(n_m)},  \theta^{(1)},..., \theta^{(n_u)}} J(x^{(1)},\dots,x^{(n_m)}, \theta^{(1)},..., \theta^{(n_u)})</script><p>where <script type="math/tex">J(x^{(1)},\dots,x^{(n_m)}, \theta^{(1)},..., \theta^{(n_u)}) = \\ \frac{1}{2} 
\color{Blue}{ \sum_{(i,j):r(i,j)=1}\left((\theta^{(j)})^T x^{(i)}-y^{(i,j)}\right)^2 } + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum^n_{k=1} (\theta^{(j)}_k)^2 + \frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n(x_k^{(i)})^2</script></p>
<p>Note that the constant term <script type="math/tex">x_0</script>‘s and <script type="math/tex">\theta_0</script>‘s are no longer needed so are removed.</p>
<p>In the algorithm we described, it is necessary to initialize x’s and θ’s to small random values to serves as <em>symmetry breaking</em> (similar to the random initialization of a neural network’s parameters) and ensures the algorithm learns features x’s that are different from each other.  </p>
<h4 id="Summarised-procedure"><a href="#Summarised-procedure" class="headerlink" title="Summarised procedure"></a>Summarised procedure</h4><ol>
<li>Initialize <script type="math/tex">x^{(1)},\dots,x^{(n_m)},  \theta^{(1)},..., \theta^{(n_u)}</script> to small random values.</li>
<li>Minimize <script type="math/tex">J(x^{(1)},\dots,x^{(n_m)}, \theta^{(1)},..., \theta^{(n_u)})</script> using gradient descent or advanced optimization algorithm. E.g. <script type="math/tex; mode=display">\forall \ j = 1,..., n_u, i = 1,...,n_m : \\ x^{(i)}_k := x^{(i)}_k - \alpha \left ( \sum_{j: r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) \ \theta^{(j)} + \lambda x^{(i)}_k \right ) \\ \theta^{(j)}_k := \theta^{(j)}_k - \alpha \left ( \sum_{i: r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) x^{(k)} + \lambda \theta^{(j)}_k \right )</script></li>
<li>For a user with parameters θ and a movie with (learned) features x, predict a star rating of <script type="math/tex">\theta^T x</script>.</li>
</ol>
<h3 id="Vectorization-Low-rank-matrix-factorization"><a href="#Vectorization-Low-rank-matrix-factorization" class="headerlink" title="Vectorization: Low rank matrix factorization"></a>Vectorization: Low rank matrix factorization</h3><p>It is intuitive to see that the whole process can be vectorised, we take the moving rating status as an example:</p>
<script type="math/tex; mode=display">Y = \begin{bmatrix}
5 & 5 & 0 & 0 \\
5 & ? & ? & 0 \\
? & 4 & 0 & ? \\
0 & 0 & 5 & 4 \\
0 & 0 & 5 & 0
\end{bmatrix}</script><script type="math/tex; mode=display">\displaystyle h(x) = \begin{bmatrix} (x^{(1)})^T(\theta^{(1)}) & \ldots & (x^{(1)})^T(\theta^{(n_u)})\\ \vdots & \ddots & \vdots \\ (x^{(n_m)})^T(\theta^{(1)}) & \ldots & (x^{(n_m)})^T(\theta^{(n_u)})\end{bmatrix}</script><p>By vectorizing <script type="math/tex">X = \begin{bmatrix} - & (x^{(1)})^T & - \\ & \vdots & \\ - & (x^{(n_m)} & - \end{bmatrix},\ \Theta = \begin{bmatrix} - & (\theta^{(1)})^T & - \\ & \vdots & \\ - & (\theta^{(n_u)} & - \end{bmatrix}</script>, we are able to write the hypothesis in a much more beatiful way:</p>
<script type="math/tex; mode=display">h(X) = X\Theta^T</script><p>This matrix is also called a low rank matrix.</p>
<h4 id="Finding-related-movies"><a href="#Finding-related-movies" class="headerlink" title="Finding related movies"></a>Finding related movies</h4><p>For each movie i , we learn a feature vector <script type="math/tex">x^{(i)} \in \mathbb{R}^n</script>. E.g. <script type="math/tex">x^{(i)}_1</script> for romance, <script type="math/tex">x^{(i)}_2</script> for action, etc.</p>
<p>Therefore, in order to find whether movie j is similar to movie i, we can compare their distance:</p>
<script type="math/tex; mode=display">\| x^{(i)} - x^{(j)} \|</script><p>such that small distance would reasonably suggest similarity. So to find 5 most similar movies to movie i, just find the 5 movies with the smallest distance.</p>
<h4 id="Mean-normalization"><a href="#Mean-normalization" class="headerlink" title="Mean normalization"></a>Mean normalization</h4><p>In the case where users have not rate any movie yet, the algorithm would predict these users to rate 0 for all movies as that minimise the cost function. Mean normalization is helpful to avoid this to happen by replacing <script type="math/tex">X\Theta^T</script> with <script type="math/tex">X\Theta^T + \mu</script> where μ is the means of movie ratings rated by some other users.</p>
<p>However, unlike some other applications of feature scaling, we did not scale the movie ratings by dividing by standard deviation. This is because all the movie ratings are already comparable (e.g. 0 to 5 stars), so they are already on similar scales.</p>

            
          </div>
          <div class="archive-post-info">
            <time class="archive-post-time">
              2019-07-25
            </time>
            <a class="archive-post-link" href="/2019/07/25/ML-part6/">
              ML-part6
            </a>
          </div>
        </div>
      
        
          
          
          
        <div class="archive-post">
          <div class="archive-post-content">
            
              <p>Contents: Unsupervised learning algorithm —- K-means and Principal Component Analysis</p>
            
          </div>
          <div class="archive-post-info">
            <time class="archive-post-time">
              2019-07-21
            </time>
            <a class="archive-post-link" href="/2019/07/21/ML-part5/">
              Notes for Coursera Machine Learning Week 8
            </a>
          </div>
        </div>
      
        
          
          
          
        <div class="archive-post">
          <div class="archive-post-content">
            
              <p>Contents: Support Vector Machine</p>
<p>BAD NEWS: Sadly, Couresara is no longer providing reading notes from this chapter and onwards for some reason (e.g. laze). That means I have to make the ENTIRE study notes by myself.</p>
            
          </div>
          <div class="archive-post-info">
            <time class="archive-post-time">
              2019-07-18
            </time>
            <a class="archive-post-link" href="/2019/07/18/ML-part4/">
              Notes for Coursera Machine Learning Week 7
            </a>
          </div>
        </div>
      
        
          
          
          
        <div class="archive-post">
          <div class="archive-post-content">
            
              <p>Contents: Evaluation and diagnosis for hypothesises.</p>
            
          </div>
          <div class="archive-post-info">
            <time class="archive-post-time">
              2019-07-16
            </time>
            <a class="archive-post-link" href="/2019/07/16/ML-part3/">
              Notes for Coursera Machine Learning Week 6
            </a>
          </div>
        </div>
      
        
          
          
          
        <div class="archive-post">
          <div class="archive-post-content">
            
              <p>Contents: Neural Networks.</p>
            
          </div>
          <div class="archive-post-info">
            <time class="archive-post-time">
              2019-07-08
            </time>
            <a class="archive-post-link" href="/2019/07/08/ML-Coursera-p2/">
              Notes for Coursera Machine Learning Week 4-5
            </a>
          </div>
        </div>
      
        
          
          
          
        <div class="archive-post">
          <div class="archive-post-content">
            
              <p>some tips for MATLAB<br>
            
          </p></div>
          <div class="archive-post-info">
            <time class="archive-post-time">
              2019-07-03
            </time>
            <a class="archive-post-link" href="/2019/07/03/tips-for-MATLAB/">
              MATLAB related
            </a>
          </div>
        </div>
      
        
          
          
          
        <div class="archive-post">
          <div class="archive-post-content">
            
              <p>Contents: Linear Regression, logistic regression, gradient descent and more.</p>
            
          </div>
          <div class="archive-post-info">
            <time class="archive-post-time">
              2019-07-02
            </time>
            <a class="archive-post-link" href="/2019/07/02/ML-Coursera-p1/">
              Notes for Coursera Machine Learning Week 1-3
            </a>
          </div>
        </div>
      
      
        
      
  </div>
  
  <nav class="pagination">
      <span class="page-number current">1</span>
  </nav>




        </div>
      </div></div></main>
      <footer class="footer syuanpi fadeIn" id="footer">
  <hr>
  <div class="footer-wrapper">
    <div class="left">
      <div class="contact-icon">
    
    
    
    
    
    
    
    
    
        
            <a href="https://github.com/oalvay" class="iconfont icon-social-github" title="github"></a>
        
        
        
        
        
        
        
        
    
</div>

    </div>
    <div class="right">
      <div class="copyright">
    <div class="info">
        <span>&copy;</span>
        <span>2019 ~ 2019</span>
        <span>❤</span>
        <span>oalvay</span>
    </div>
    <div class="theme">
        <span>
            Powered by
            <a href="http://hexo.io/" target="_blank">Hexo </a>
        </span>
        <span>
            Theme
            <a href="https://github.com/ColMugX/hexo-theme-Nlvi"> Nlvi </a>
        </span>
    </div>
    
</div>

    </div>
  </div>
</footer>
    </div>
    <div class="tagcloud" id="tagcloud">
  <div class="tagcloud-taglist">
  
    <div class="tagcloud-tag">
      <button>主线剧情</button>
    </div>
  
    <div class="tagcloud-tag">
      <button>技多不压身</button>
    </div>
  
  </div>
  
    <div class="tagcloud-postlist active">
      <h2>主线剧情</h2>
      
        <div class="tagcloud-post">
          <a href="/2019/06/17/hello-world/">
            <time class="tagcloud-posttime">2019 / 06 / 17</time>
            <span>Hello Hexo</span>
          </a>
        </div>
      
    </div>
  
    <div class="tagcloud-postlist ">
      <h2>技多不压身</h2>
      
        <div class="tagcloud-post">
          <a href="/2019/06/17/hexo-github-搭建博客小记/">
            <time class="tagcloud-posttime">2019 / 06 / 17</time>
            <span>hexo+github 搭建博客小记</span>
          </a>
        </div>
      
    </div>
  
</div>

  </div>
  <div class="backtop syuanpi melt toTop" id="backtop">
    <i class="iconfont icon-up"></i>
    <span style="text-align:center;font-family:Georgia;"><span style="font-family:Georgia;" id="scrollpercent">1</span>%</span>
</div>


<script src="/script/lib/jquery/jquery-3.2.1.min.js"></script>


  <script src="/script/lib/lightbox/js/lightbox.min.js"></script>



  <script src="https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/x-mathjax-config;executed=true">MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"], ["\\(","\\)"]]}});</script>







  <script src="/script/scheme/banderole.js"></script>


<script src="/script/bootstarp.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</body>
</html>
