---
title: Summary Note - Decision and Risk  
date: 2020-02-20 15:30:00
tags:
- Notes
---

I recently came up with an idea. Insteading of summarizing lectures by hands, how about using markdown and post on blog?

<!-- more -->

<b><font size = 6>Recap</font></b>

By conditioning on the known value of the data y and using Bayes’ theorem, we yield the **posterior density**:

$$p(\theta|\mathbf{y}) = \frac{p(\theta, \mathbf{y})}{p(\mathbf{y})} = \frac{p(\theta)p(\mathbf{y}|\theta)}{p(\mathbf{y})}$$

where:  

+ $$p(\theta∣\mathbf{y})$$ - is the **posterior density** for θ and represents the uncertainty about θ after conditioning on the data y.
+ $$p(\theta)$$ - is the **prior density** for θ that expresses our uncertainty about the values of θ before taking into account sample information (i.e. observed data).
+ $$p(\mathbf{y}|\theta)$$  - when regarded as a function of θ, for fixed y, is the **likelihood function**.
+ $$p(\mathbf{y})$$ - is the marginal density of the data y, normally written as:

$$p(\mathbf{y}) = \underbrace{\sum_{\theta} p(\theta)p(\mathbf{y}|\theta)}_{\mbox{discrete} \ \theta} = \underbrace{\int_{\theta} p(\theta)p(\mathbf{y}|\theta) \ d\theta}_{\mbox{continuous} \ \theta}$$

Or, by omitting the normalizing constant $$p(\mathbf{y})$$,we have the **unnormalized posterior density**:

$$\begin{align*}
p(\theta∣\mathbf{y}) 
& \propto p(\theta) \cdot p(\mathbf{y}|\theta) \\
& \propto \text{prior pdf ⋅ likelihood function}
\end{align*}$$ 

The parameters that controls prior distrition are called **hyperparameters**.

---
In a Bayesian analysis, we first need to represent our prior beliefs about θ, by constructing a probability distribution p(θ) which encapsulates our beliefs.  

p(θ) will not be the same for different people as they may have different knowledge about what proportion of coins are biased.

In some cases, p(θ) may be based on subjective judgement, while in others it may be based on objective evidence. This is the essence of Bayesian statistics - probabilities express degrees of beliefs.

# Decision Theory

## Bayesian decision theory

\- concerned with making decisions that perform best, based on the information we have about the un- knowns.

#### Basic Elements of a Decision Problem

- &nbsp;$$\Theta$$ is the ***parameter space*** which consists of all possible “states of nature” or “states of the world” $$\theta$$, only one of which will occur. The “true” state of nature $$\theta$$ is unknown.

- &nbsp;$$A = (a_1,a_2,...,a_k)$$ is the ***action space***, which is the set of all pos- sible actions available, $$a \in A$$ .

- &nbsp;$$\Omega$$ contains all possible realisations $$y \in \Omega$$ of a random variable $$Y$$ which belongs to the family $$\{ f(y; \theta)∣ \theta \in \Theta \} $$

- &nbsp;$$L(\theta, a)$$ is a ***loss function*** that has the domain $$Θ \times A = \{ (θ, a)∣ θ ∈ Θ \ \text{and} \ a ∈ A \}$$ and codomain $$\mathbb{R}$$. That is, a loss function maps each combination of states of the world θ and action a onto a numerical loss, R. For technical convenience, $$L(θ, a) ≥ −K > −∞$$

#### Loss Function

The loss function $$L(\theta,a)$$ is a core element of decision making which represents the loss incurred if we choose action a when the true state of the world is $$\theta$$ (usually unknown).

The losses corresponding to each action and state of world θ can be represented by a loss matrix:

 &nbsp;|$$\theta = 0$$&nbsp;|$$\theta = 1$$ &nbsp;
---|---|---
$$a_1$$&nbsp;|0|10
$$a_0$$&nbsp;|1|0
which fully specifies the loss function $$L(\theta,a)$$ for all values of θ and a.

If $$\pi^*(\theta)$$ is the believed probability distribution of θ at the time of decision making, the **Bayesian expected loss** of an action $$a$$ is:

$$\rho(\pi^*, a) = E^{\pi^*}L(\theta, a) = \int_{\Theta}L(\theta, a)\ dF^{\pi^*}(\theta)$$

where $$F$$ is the c.d.f of the random variable X.

#### The conditional Bayes Principle

Choose an action $$a \in A$$ which minimizes $$\rho(\pi^*, a)$$ (assuming the minimum is attained). Such an action will be called a **Bayes action** and will be denoted $$a^{\pi^*}$$

Note: In a multiclass classification, we shall use the One vs Rest method to keep things simple.

<table><td bgcolor=#87CEFA></td></table>
---
## Frequentist Risk

A **decision rule** $$\delta(x)$$ is a function from $$\Omega$$ into $$A$$. Given a particular realization $$X = x$$, $$\delta(x)$$ is the action that will be taken. 

Two decision rules, $$\delta_1$$ and $$\delta_2$$, are said to be <u>equivalent</u> if $$P_\theta (\delta_1(X) = \delta_1(X)) = 1$$ for all θ.

The **risk function** of a decision rule $$\delta(x)$$ is defined by:

$$R(\theta, \delta) = E^X_\theta[L(\theta, \delta(x))] = \int_\Theta L(\theta, \delta(x)) \ dF^X(x|\theta)$$

It is natural to use a decision rule $$\delta(x)$$ which has smallest risk $$R(\theta, \delta)$$. However, in contrast to the Bayesian expected loss, the risk is a function of θ, and hence it is not a single number.  
Since θ is unknown, the meaning of “smallest” is not clearly defined, so we need another way to choose decision.

A decision rule $$\delta_1$$, is **R-better** than a decision rule $$\delta_2$$, if $$R(\theta, \delta_1) ≤ R(\theta, \delta_2)$$ for all $$\theta ∈ \Theta$$, with strict inequality for some θ. A decision rule $$\theta_1$$, is **R-equivalent** to a decision rule $$\theta_2$$, if $$R(\theta, \delta_1) = R(\theta, \delta_2)$$ for all $$\theta ∈ \Theta$$.


A decision rule $$\delta$$ is said to be **admissible** if there does not exist R-better decision rule. A decision rule $$\delta$$ is **inadmissible** if there does exist an R-better decision rule.

It's clear that we shall never use an inadmissible decision rule, but the class of admissible decision rules for a given decision problem can be large. This means that there will be admissible rules with risk functions $$R(\theta, \delta)$$ that are “better” in some regions of the parameter space $$\Theta$$, and “worse” in others, i.e. *risk functions cross*.









